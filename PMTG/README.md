# Policies Modulating Trajectories (PMTG)
This is an implementation of policies modulating trajectories (PMTG) as described in this [paper](https://arxiv.org/abs/1910.02812). I implemeted the algorithm for the toy problem the authors describe and also for a monoped robot in PyBullet. I created an OpenAI gym for the monoped robot. 

# Issues Addressed by Policies Modulating Trajectory Generators 
Deep learning for control in simulated environments is an exciting area of research. Many people have succesfully learned policies in simulated environments and then succesfully deployed them onto real hardware. Although, these methods require a ton of computing resources and often learn brittle control policies. Researchers have tried to address this by using more complicated function approximators like recurrent neural networks. Another approach has been to try to incorporate prior knowledge into the machine learning problem. Some have used classical controllers and then used RL on top of it to some success. For example, you might have a classical controller's output be modulated by a neural network running DDPG to learn how to control a robot's motors. PMTG is very similiar to this but takes it a step further. PMTG has an RL algorithm learn how to control not the robot directly but a new dynamical system - that of the robot and its trajectory generator. Our trajectory generator has some internal parameters it uses to compute a series of states the robot should be in. The controller could be some sort of optimal controller that requires in depth modelling of the environment and its dynamics or simply some sort of function that capture's the designer's intuiton about what a stable trajectory might look like. From experience with legged animals, we have a general idea about what the motion of a quadruped's legs is like. We want to avoid having the neural networks learn the entire control policy from scratch. So, PMTG lets the overall control signal that is sent to the robot be a sum of the trajectory generator and a modulation term from the neural network. PMTG also lets the RL agent modulate the internal parameters of the trajectory generator. The RL agent is learning how to control the system that is the robot, and its controller. This is in constrast to Vanilla DDPG, TD3, or PPO where we directly try to learn how to control the robot. 


# Results on PMTG Paper Problem 
The PMTG author's describe a toy problem for illustrating their approach. Suppose we have perfect position control over a ball moving on a plane and we had some sort of deformed figure-eight path we wanted the ball to follow over time. Since we know the optimal path for the ball is roughly a figure eight shape, we could create a trajectory generator that leads to a figure eight shape. This figure eight shape would have parameters controlling the width and height of the loops. At each time step, the trajectory generator outputs an (x, y) position for the ball. The RL agent can modulate this (x, y) control signal and also set the internal parameters of the trajectory generator. A control diagram for the system is below. <br />
![](media/controlDiagram.png) <br />

The author's optimal path and trajectory generators are shown below <br />
![](media/optimalPath_and_TGs.png) <br />

I created a similiar environment and used a PMTG architecture with DDPG to learn a path through the environment. The blue, warped figure eight is the optimal path and the orange path is the learned path. 
Reward Per Epoch over Time |  Learned Policy
:-------------------------:|:-------------------------:
![](media/loss.png)  | ![](media/learnedPath.png)

# H3pper Robot PMTG 
The optimal wrench controller takes in the initial state of the robot and outputs an open loop controller specifying what wrenches for the robot's foot to apply in order to have a stable forward gait. The open loop controller was developed by Dan Lynch and requires a Matlab collocation package. Moreover, it requires a good initial guess in order to find a solution. So, we generated a single open loop wrench generator from Dan's model. It can be thought of as being defined by three curves. Each curve is a single component of the planar wrench vector over time. Each curve has N points that define what that wrench component as time progresses. N becomes another hyper-parameter to be tuned. The agent can move each of these N points within a small region. How big a region to allow the agent to change the wrench generator's points becomes another hyper-parameter to be tuned. This gives the agent a way to efficiently modify the underlying wrench generator. Then, given a time, for each curve, we can interpolate between the nearest two points. This allows us to efficiently map a time to a wrench with Dan's optimal controller. So, the open loop optimal controller outputs a planar wrench at each time step that the policy network can further modulate with residual terms. This planar wrench is then mapped down to a set of joint torques for the robot to apply. The policy network then uses DDPG to learn how to set the wrench generator parameters and apply resdidual terms. Below is a flowchart of the architecture. <br /> 
![](media/flowchart.jpg) 

## Optimal Wrench Controller 
Dan Lynch has done a lot of amazing work creating optimal controllers for legged robots on soft ground. His controllers are open loop and map a time to a wrench for the robot's foot to apply. Below is a plot of the wrench controller over one period of the gait, that is one hop phase and one flight phase. During the flight phase, the robot's foot applies the zero wrench. <br />
![](media/optimal_controller_plots.png)

# Results
I have yet to get the robot to learn to hop with the PMTG architecture but we will keep trying. 